{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"KB2YHrqq6Pmv"},"source":["#Подготовка"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NNPd7sBx5zwX"},"source":["##Импорт и установка библиотек"]},{"cell_type":"code","execution_count":123,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7346,"status":"ok","timestamp":1686296682366,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"Xkf_RJnl_sGb","outputId":"74079e03-349b-4353-b106-842fc486d612"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pydicom in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["# Установка pydicom\n","%pip install pydicom"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nibabel in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.1.0)\n","Requirement already satisfied: numpy>=1.19 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nibabel) (1.23.5)\n","Requirement already satisfied: packaging>=17 in c:\\users\\maxhari\\appdata\\roaming\\python\\python310\\site-packages (from nibabel) (23.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install nibabel"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.12.0)\n","Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n","Requirement already satisfied: jax>=0.3.15 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.11)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n","Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\maxhari\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.2)\n","Requirement already satisfied: setuptools in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (63.2.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\maxhari\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.6.3)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n","Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n","Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n","Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n","Requirement already satisfied: scipy>=1.7 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.19.1)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n","Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.3)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n","Requirement already satisfied: urllib3<2.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.16)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2023.5.7)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install tensorflow"]},{"cell_type":"code","execution_count":126,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: matplotlib in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7.1)\n","Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.7)\n","Requirement already satisfied: cycler>=0.10 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.39.4)\n","Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n","Requirement already satisfied: numpy>=1.20 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\maxhari\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (23.0)\n","Requirement already satisfied: pillow>=6.2.0 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\maxhari\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\maxhari\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: six>=1.5 in c:\\users\\maxhari\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install matplotlib"]},{"cell_type":"code","execution_count":127,"metadata":{"id":"ia_--NvLmQsG"},"outputs":[],"source":["# Модель\n","from tensorflow.keras import Model, Input, regularizers\n","\n","# Загрузка модели\n","from tensorflow.keras.models import load_model\n","\n","# Слои\n","from tensorflow.keras.layers import Conv3D, BatchNormalization, Activation, Add\n","from tensorflow.keras.layers import MaxPooling3D, Conv3DTranspose, concatenate\n","\n","# Оптимизаторы\n","from keras.optimizers import Adam\n","\n","# Метрики\n","from tensorflow.keras.metrics import BinaryIoU\n","\n","# Регуляризация параметров модели\n","from tensorflow.keras.regularizers import l2\n","\n","# Схема модели\n","from tensorflow.keras.utils import plot_model\n","\n","#Для сохранения и загрузки модели с кастомными метриками\n","#from keras.utils.generic_utils import get_custom_objects\n","\n","# Для низкоуровневого доступа к операциям и тензорам\n","import tensorflow.keras.backend as K\n","\n","# Для работы с изображениями\n","from PIL import Image\n","\n","# Для работы с dcm-изображениями\n","import pydicom as dicom\n","\n","# Для работы с nii-изображениями\n","import nibabel as nib\n","\n","# Инструменты для работы с массивами\n","import numpy as np\n","\n","# Для уменьшения размеров изображений в numpy\n","from scipy.ndimage import zoom\n","\n","# Для работы с файлами\n","import os, shutil\n","\n","# Для работы с датой/временем\n","import datetime\n","\n","# очистка ОЗУ\n","import gc\n","\n","# Отрисовка графиков\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z6s_rdCe_gVP"},"source":["##Глобальные параметры"]},{"cell_type":"code","execution_count":128,"metadata":{"id":"JLZJoZKsvgrE"},"outputs":[],"source":["CLASS_COUNT = 1                                             # Количество классов (с выпотом и без, бинарная сегментация)\n","MIN_BOUND = 0                                               # Минимальная граница расположения выпота\n","MAX_BOUND = 400                                             # Максимальная граница расположения выпота\n","MAX_SLICES = 304                                            # Максимальное количество срезов\n","PROJECT_DIR = 'D:\\\\Projects\\\\AIU\\\\2_Radlogics'              # Папка с проектом\n","DATA_DIR = os.path.join(PROJECT_DIR, 'datasets')            # Папка с данными\n","LEARNING_DIR = os.path.join(PROJECT_DIR, 'learning')        # Чекпоинты\n","MODEL = 0                                                   # Модель: 0 - U-Net 3D, 1 - VoxResNet\n","IMG_RESOLUTION = 512                                        # Разрешение среза\n","BATCH_SIZE = 1                                              # Размер батча\n","SIZE_STEP = BATCH_SIZE                                      # Шаг смещения\n","LAST_EPOCH = 0                                              # Последняя эпоха, применяется при дообучении модели\n","\n","# Управляющие флаги\n","CLASS_EQUATING_f = True                                     # Уравнивание классов\n","VALIDATION_f = True                                         # Использование проверочной выборки при обучении\n","FULL_DATASET_f = True                                       # Полный датасет или частичный\n","PNG_MASKS_f = True                                          # Использование сегментированных 2D изображений в формате png вместо 3D в nii.gz\n","SIMPLE_3D_f = False                                         # Переключение полной и упрощённой 3D сегментации // не реализовано\n","AUGMENTATION_f = False                                      # Флаг применения аугментации // не реализовано\n","ADDITIONAL_TRAIN_f = False                                  # Режим дообучения, с загрузкой весов\n","\n","# Пути к папкам с оригинальными изображениями и плевральными масками\n","original_dir = os.path.join(DATA_DIR, 'Original')   # Папка с оригинальными DICOM файлами\n","effusions_dir = os.path.join(DATA_DIR, 'Effusions')         # Папка с файлами плевральных выпотов\n","thor_cav_dir = os.path.join(DATA_DIR, 'Thoracic_Cavities')  # Папка с файлами грудных полостей"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tzyolrdb2zEE"},"source":["##Полезные функции\n","\n","\n","*   get_full_path(sub_path)\n","*   get_folders(path)\n","*   plot_pics(imgs, titles)\n","*   Model checkpoints"]},{"cell_type":"code","execution_count":129,"metadata":{"id":"dL4jY1lMWoU-"},"outputs":[],"source":["# Функция получения пути к папке с изображениями, будь то:\n","# LUNG1-XXX;\n","# LUNG1-XXX/какая-то_папка\n","# LUNG1-XXX/какая-то_папка1/какая-то_папка2\n","\n","def get_full_path(sub_path):  # sub_path - полный путь к папке LUNG1-XXX\n","\n","    full_path = sub_path\n","    for _ in range(2):\n","        if len(os.listdir(full_path)) > 0:\n","            if os.path.exists(os.path.isdir(os.path.join(full_path, os.listdir(full_path)[0]))):\n","                path = os.path.join(full_path, os.listdir(full_path)[0])\n","                if os.path.isdir(path): full_path = path\n","\n","    return full_path"]},{"cell_type":"code","execution_count":130,"metadata":{"id":"x77714EXWi6H"},"outputs":[],"source":["# Функция получения списка исключительно папок в определённой директории\n","\n","def get_folders(path):                        # путь к папке с данными\n","    # Создаём список, содержащий только папки\n","    folders = []\n","    for item in os.listdir(path):             # список элементов в директории\n","        full_path = os.path.join(path, item)\n","        if os.path.isdir(full_path):\n","            folders.append(item)\n","\n","    return folders"]},{"cell_type":"code","execution_count":131,"metadata":{"id":"5ByI32jhCOTK"},"outputs":[],"source":["# Функция для отображения изображений\n","\n","def plot_pics(imgs, titles):\n","\n","    num = len(imgs)\n","    plt.figure(1, figsize=(3*num, 3))\n","\n","    for i in range(num):\n","        plt.subplot(1, num, i+1)\n","        plt.title(titles[i])\n","        plt.imshow(imgs[i], cmap='gray')\n","        plt.axis('off')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":132,"metadata":{"id":"N_RrQ4PFb152"},"outputs":[],"source":["# Функция для сохранения директорий данных и масок выборок в файл\n","\n","def save_parameters(model_name, train, val, test):\n","\n","    with open(os.path.join(LEARNING_DIR, f'{model_name}_parameters.txt'), mode=\"a\") as file:\n","        # получаем текущее время\n","        current_time = datetime.datetime.now()\n","        \n","        # записываем дату и время\n","        file.write(f'Произведено: {current_time}')\n","\n","        # записываем наименования директорий\n","        file.write(f'\\nДиректории оригинальных изображений:\\n')\n","        file.write(' '.join(map(str, orig_dir_np)))\n","        file.write(f'\\nДиректории масок с выпотами:\\n')\n","        file.write(' '.join(map(str, eff_dir_np)))\n","\n","        # записываем маски\n","        file.write(f'\\nМаска для обучающей выборки:\\n')\n","        file.write(' '.join(map(str, train.astype(np.byte))))\n","        file.write(f'\\nМаска для проверочной выборки:\\n')\n","        file.write(' '.join(map(str, val.astype(np.byte))))\n","        file.write(f'\\nМаска для тестовой выборки:\\n')\n","        file.write(' '.join(map(str, test.astype(np.byte))) + '\\n')\n","\n","\n","# Функции сохранения весов и полной модели\n","\n","def save_model(model_name, epoch=0):\n","    model.save(os.path.join(LEARNING_DIR, f'{model_name}_{epoch}_model.h5'))\n","\n","def save_weights(model_name, epoch=0):\n","    model.save_weights(os.path.join(LEARNING_DIR, f'{model_name}_{epoch}_weights.h5'))\n","\n","\n","# Функция для загрузки директорий данных и масок выборок в файл\n","\n","def load_parameters(model_name):\n","    parameters = []\n","    with open(os.path.join(LEARNING_DIR, model_name+'_parameters.txt'), 'r') as file:\n","        for i, line in enumerate(file):\n","            # считываем orig_dir_np, eff_dir_np, train_mask, val_mask, test_mask\n","            if i != 0 and i % 2 == 0:\n","                if i <= 4: parameters.append(np.array(line.split()))\n","                else: parameters.append(np.array(line.split()).astype(bool))\n","    return parameters\n","\n","\n","# Функции для загрузки весов и полной модели\n","\n","def load_weights(model_name, epoch=LAST_EPOCH):\n","    model.load_weights(os.path.join(LEARNING_DIR, f'{model.name}_{epoch}_weights.h5'))\n","    return model\n","\n","def load_full_model(model_name, epoch=LAST_EPOCH):\n","    model = load_model(os.path.join(LEARNING_DIR, f'{model.name}_{epoch}_model.h5'))\n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0M7vSv_UL3gK"},"source":["#Сегментация плевральных выпотов"]},{"cell_type":"code","execution_count":133,"metadata":{"id":"92mEryT8wH2P"},"outputs":[],"source":["# Определим количество частей на срезе и их разрешение при упрощённой и полной сегментации \n","chunks_count = 16 if SIMPLE_3D_f else 1\n","chunk_res = int(IMG_RESOLUTION/(chunks_count**0.5)) if SIMPLE_3D_f else IMG_RESOLUTION"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gWYQXtkIlMNQ"},"source":["##Нейронка для сегментации"]},{"cell_type":"code","execution_count":134,"metadata":{"id":"HYo4Cp99h4s3"},"outputs":[],"source":["#@title Самописная функция ошибки и метрики Dice\n","\n","def dice_coef(y_true, y_pred, shape=1e-6):\n","    y_true = K.cast(y_true, dtype='float32') # приводим y_true к типу float32\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    # print(f'y_true= {type(y_true)} y_pred= {type(y_pred)}')\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    calc1 = (2. * intersection + shape)\n","    calc2 =  (K.sum(y_true_f) + K.sum(y_pred_f) + shape)\n","    res = calc1 / calc2\n","    return  res\n","\n","def dice_coef_loss(y_true, y_pred):\n","    return 1 - dice_coef(y_true, y_pred)\n","\n","#get_custom_objects().update({\"dice_coef\": dice_coef, \"dice_coef_loss\": dice_coef_loss})"]},{"cell_type":"code","execution_count":135,"metadata":{"cellView":"form","id":"x0Gx2hjW6pEW"},"outputs":[],"source":["#@title U-Net 3D Model\n","\n","def conv3d_block(input_tensor, num_filters):\n","    x = Conv3D(num_filters, 3, padding=\"same\")(input_tensor)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    x = Conv3D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)\n","    x = Activation(\"relu\")(x)\n","\n","    return x\n","\n","def encoder_block(input_tensor, num_filters):\n","    e_out = conv3d_block(input_tensor, num_filters)\n","    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(e_out)\n","    return e_out, x\n","\n","def decoder_block(input_tensor, skip_tensor, num_filters):\n","    x = Conv3DTranspose(num_filters, kernel_size=(2, 2, 2), strides=(2, 2, 2), padding=\"same\")(input_tensor)\n","    x = concatenate([x, skip_tensor], axis=-1)\n","    x = conv3d_block(x, num_filters)\n","    return x\n","\n","def unet_3d(input_shape, num_classes):\n","    inputs = Input(input_shape)\n","\n","    # Encoder\n","    e_out1, e1 = encoder_block(inputs, 64)\n","    e_out2, e2 = encoder_block(e1, 128)\n","    e_out3, e3 = encoder_block(e2, 256)\n","    e_out4, e4 = encoder_block(e3, 512)\n","\n","    # Bridge\n","    b1 = conv3d_block(e4, 1024)\n","\n","    # Decoder\n","    d1 = decoder_block(b1, e_out4, 512)\n","    d2 = decoder_block(d1, e_out3, 256)\n","    d3 = decoder_block(d2, e_out2, 128)\n","    d4 = decoder_block(d3, e_out1, 64)\n","\n","    # Output\n","    outputs = Conv3D(num_classes, 1, activation=\"sigmoid\", padding=\"same\")(d4)\n","\n","    model = Model(inputs, outputs, name = 'unet_3d')\n","    return model"]},{"cell_type":"code","execution_count":136,"metadata":{"cellView":"form","id":"yT1OAsfJb915"},"outputs":[],"source":["#@title VoxResNet 3D Model\n","\n","def vox_resnet(input_shape, classes, n=[3, 4, 6, 3], k=32):\n","\n","    def conv(filters):\n","        return Conv3D(filters, kernel_size=3, strides=1, padding='same',\n","                      kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1.e-4))\n","\n","    def deconv(filters):\n","        return Conv3DTranspose(filters, kernel_size=3, strides=2, padding='same',\n","                               kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1.e-4))\n","\n","    def residual_block(x, filters):\n","        skip = x\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = conv(filters)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = conv(filters)(x)\n","        x = Add()([x, skip])\n","        return x\n","\n","    def upsampling_block(x, filters):\n","        x = deconv(filters)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        return x\n","\n","    def downsample_block(x, filters):\n","        x = conv(filters)(x)\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = MaxPooling3D(pool_size=(2, 2, 2))(x)\n","        return x\n","\n","    inputs = Input(shape=input_shape)\n","    x = inputs\n","\n","    for i in range(n[0]):\n","        x = residual_block(x, k)\n","    x = downsample_block(x, k*2)\n","\n","    for i in range(n[1]):\n","        x = residual_block(x, k*2)\n","    x = downsample_block(x, k*4)\n","\n","    for i in range(n[2]):\n","        x = residual_block(x, k*4)\n","    x = upsampling_block(x, k*2)\n","\n","    for i in range(n[3]):\n","        x = residual_block(x, k*2)\n","    x = upsampling_block(x, k)\n","\n","    x = residual_block(x, k)\n","    outputs = Conv3D(classes, kernel_size=3, strides=1, padding='same',\n","                     kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(1.e-4), activation='softmax')(x)\n","\n","    model = Model(inputs=inputs, outputs=outputs, name = 'voxresnet')\n","    return model"]},{"cell_type":"code","execution_count":137,"metadata":{"id":"ei5yvTi_KBVx"},"outputs":[],"source":["#@title Создание и компиляция модели\n","\n","input_shape = (chunk_res, chunk_res, MAX_SLICES, 1)\n","optimizer=Adam(learning_rate=0.0005)\n","\n","if MODEL == 0:\n","    model = unet_3d(input_shape, CLASS_COUNT)\n","else:\n","    model = vox_resnet(input_shape, CLASS_COUNT)\n","\n","model.compile(optimizer=optimizer,\n","              loss=dice_coef_loss,\n","              metrics=[[BinaryIoU(target_class_ids=[0, 1], threshold=0.9, name='binary_io_u')], dice_coef])"]},{"cell_type":"code","execution_count":138,"metadata":{"cellView":"form","id":"UhmjomRMJ7Xy"},"outputs":[],"source":["#@title Смотрим характеристики модели\n","\n","#model_unet_3d.summary()\n","\n","# Выводим схему модели\n","\n","#plot_model(model_unet_3d)"]},{"cell_type":"code","execution_count":139,"metadata":{"cellView":"form","id":"qnrhOZ-DEfrc"},"outputs":[],"source":["#@title Вывод логов обучения\n","\n","def print_log(current,                           # номер текущего батча\n","              amount,                            # число всех батчей \n","              params):                           # словарь дополнительных параметров для вывода \n","  \n","  bar_len = 20                                   # длина бара \n","  percent = int(current * bar_len / amount)      # процент выполненной работы\n","  progressbar = ''\n","\n","  for i in range(bar_len):                       # Проходим по всем элементам прогрессбара и добавляем символы в соответствии с прогрессом \n","    if(i < percent):\n","      progressbar += '='\n","    elif(i == percent):\n","      progressbar += '>'\n","    else:\n","      progressbar += '-'\n","\n","  # Добавляем в финальное сообщение символ переноса каретки консоли на начальную строку,\n","  # добавляем информацию о номере батча, количестве всех батчей, прогрессбар.\n","  # Символ переноса каретки \\r добавляется для того, чтобы каждый новый батч перезаписывать\n","  # вывод - таким образом он не будет засоряться повторяющейся информацией.\n","  message = \"\\r\" + str(current) + '/' + str(amount) + ' [' + progressbar + ']  ' \n"," \n","  # Добавляем дополнительные параметры в вывод\n","  for key in params: message += key + ': ' + str(round(params[key], 4)) + '. '\n","  \n","  print(message, end='')"]},{"cell_type":"code","execution_count":140,"metadata":{"cellView":"form","id":"iWFEopq09KCM"},"outputs":[],"source":["#@title Коллбэки\n","\n","def callbacks(model, model_name, epoch, params, callback, optimizer = optimizer,\n","              min_delta=1e-3, min_lr=1e-6, max_lr=1e-4, es_patience=8, lr_patience=5, lr_factor=0.7):\n","\n","    def model_checkpoint():\n","        if accuracy > callback['mc_accuracy'] and loss < callback['mc_loss']:\n","            print(f\"В сравнении с эпохой {callback['mc_epoch']+1}: {add_name}loss уменьшилось\", end=' ')\n","            print(f\"с {round(callback['mc_loss'],4)} до {loss}, а {add_name}accuracy увеличилось с\", end=' ')\n","            print(f\"{round(callback['mc_accuracy'],4)} до {accuracy}.\")\n","            callback['mc_loss'] = loss\n","            callback['mc_accuracy'] = accuracy\n","            callback['mc_epoch'] = epoch\n","            model.save_weights(os.path.join(GDRIVE_PATH, DRIVE_DATA_DIR, model_name+'_weights.h5'))\n","            model.save(os.path.join(GDRIVE_PATH, DRIVE_DATA_DIR, model_name+'_model.h5'))\n","            print(f'Модель {model_name}.h5 сохранена на эпохе {epoch+1}.')\n","\n","    add_name = 'val_' if VALIDATION_f else ''\n","    loss = round(params[add_name+'loss'][epoch], 4)\n","    accuracy = round(params[add_name+'accuracy'][epoch], 4)\n","\n","    if epoch > 0:\n","\n","        if VALIDATION_f:\n","\n","            es_count = callback['es_count']\n","            lr_count = callback['lr_count']\n","\n","            # ModelCheckpoint\n","            model_checkpoint()\n","\n","            # ReduceLRonPlateau\n","            if (callback['lr_loss'] - loss) < min_delta:\n","                lr_count += 1\n","                if lr_count >= lr_patience:                   \n","                    lr_val = round(optimizer.lr.numpy()*lr_factor, 7)\n","                    a = 'а' if lr_count in [2,3,4] else ''\n","                    print(f\"Начиная с {callback['lr_epoch']+1}-й эпохи val_loss особо не уменьшалось {lr_count} раз{a} подряд.\")\n","                    if lr_val < min_lr:\n","                        optimizer.learning_rate.assign(max_lr)\n","                        print(f'Learning rate стал ниже своего минимального значения и вновь восстановлен до {max_lr}.')\n","                    else:\n","                        optimizer.learning_rate.assign(lr_val)\n","                        print(f'Learning rate уменьшается до {lr_val}.')                 \n","                    callback['lr_loss'] = loss\n","                    callback['lr_epoch'] = epoch\n","                    lr_count = 0\n","            else:\n","                callback['lr_loss'] = loss\n","                callback['lr_epoch'] = epoch\n","                lr_count = 0\n","            callback['lr_count'] = lr_count\n","      \n","            # EarlyStopping\n","            if (callback['es_loss'] - loss) < min_delta:\n","                es_count += 1\n","                if es_count >= es_patience:\n","                    print(f\"Угроза переобучения! Восстанавливаются веса модели лучшей эпохи: {callback['mc_epoch']+1}.\")\n","                    model.load_weights(os.path.join(GDRIVE_PATH, DRIVE_DATA_DIR, model_name+'_weights.h5'))\n","                    print(f'Веса модели {model_name} восстановлены. Дальнейшее обучение приостановлено.')\n","                    es_count = -1\n","            else:\n","                es_count = 0\n","            callback['es_count'] = es_count\n","\n","        else:\n","            model_checkpoint()\n","    else:\n","        callback.update({\n","            'lr_loss': loss,\n","            'es_loss': loss,\n","            'mc_loss': loss,\n","            'mc_accuracy': accuracy\n","        })\n","        model.save_weights(os.path.join(GDRIVE_PATH, DRIVE_DATA_DIR, model_name+'_weights.h5'))\n","\n","    return model, callback"]},{"cell_type":"code","execution_count":141,"metadata":{"cellView":"form","id":"FHu0OxIOPopV"},"outputs":[],"source":["#@title Загрузка модели при необходимости\n","if ADDITIONAL_TRAIN_f:\n","    orig_dir_np, eff_dir_np, train_mask, val_mask, test_mask = load_parameters(model.name)\n","    model = load_weights(model.name)\n","    # model = load_full_model(model.name, LAST_EPOCH)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lxyoy71OGPON"},"source":["##Получение массивов директорий"]},{"cell_type":"code","execution_count":142,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1686297878804,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"wOmLONiiJ-ko","outputId":"10b0e325-efc9-45db-d654-f2173136baf0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Все образцы находятся в следующих папках:  ['LUNG1-302' 'LUNG1-303' 'LUNG1-304' 'LUNG1-305' 'LUNG1-306']..\n","Маски выпотов хранятся в следующих папках: ['LUNG1-302' 'LUNG1-303' 'LUNG1-304' 'LUNG1-305' 'LUNG1-306']..\n","Всего экземпляров с выпотами: 121\n","Общее количество экземпляров: 121\n"]}],"source":["#@title Массивы директорий экземпляров\n","\n","# При первом обучении \n","if not ADDITIONAL_TRAIN_f:\n","    # Получаем списки папок LUNG1-XXX и переводим их в numpy массив\n","    orig_dir_np = np.array(sorted(get_folders(original_dir))) # папки с оригинальными изображениями\n","    eff_dir_np = np.array(sorted(get_folders(effusions_dir))) # папки с плевральными выпотами\n","\n","    # Отфильтровываем директории в eff_dir_np, которых нет в orig_dir_np\n","    eff_dir_np = eff_dir_np[np.in1d(eff_dir_np, orig_dir_np)]\n","\n","print(f'Все образцы находятся в следующих папках:  {orig_dir_np[:5]}..')\n","print(f'Маски выпотов хранятся в следующих папках: {eff_dir_np[:5]}..')\n","print('Всего экземпляров с выпотами:', len(eff_dir_np))\n","print('Общее количество экземпляров:', len(orig_dir_np))"]},{"cell_type":"code","execution_count":143,"metadata":{"id":"OW7AcLjdJeN7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Без выпотов до \"обработки\": 0 []\n","Общий после \"обработки\" (выпоты x2): 121 ['LUNG1-302' 'LUNG1-303' 'LUNG1-304' 'LUNG1-305' 'LUNG1-306']\n"]}],"source":["#@title Уравнивание классов при необходимости\n","\n","# Установка начального «семени» в случайные значения\n","np.random.seed()\n","\n","if CLASS_EQUATING_f and not ADDITIONAL_TRAIN_f:\n","\n","    # Получаем директории экземпляров, свободных от выпотов\n","    no_eff_dir_np = np.setdiff1d(orig_dir_np, eff_dir_np)\n","    print('Без выпотов до \"обработки\":', len(no_eff_dir_np), no_eff_dir_np[:5])\n","\n","    # Перемешиваем массив\n","    np.random.shuffle(no_eff_dir_np)\n","\n","    # Рубаем массив по количеству экземпляров с выпотами, объединяем с выпотами и сортируем\n","    orig_dir_np = np.sort(np.concatenate((no_eff_dir_np[:len(eff_dir_np)], eff_dir_np)))\n","    print('Общий после \"обработки\" (выпоты x2):', len(orig_dir_np), orig_dir_np[:5])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8dlteIEc83qU"},"source":["##Формирование train, val, test масок и массива выходных данных"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VK95lgDiXGcy"},"source":["После проведённого ряда экспериментов, связанных с ограничением памяти по формированию входного массива, принято решение формировать входные данные партиями и постепенно обучать модель."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cPDOfM8aLzHV"},"source":["Сформируем метки наличия выпотов `effusion_labels` для экземпляров (директорий) оригинальных 3D изображений путём сравнения наличия директорий `LUNG1-XXX` в папках `ORIGINAL_DIR` и `EFFUSIONS_DIR`.\n","\n","Значения `effusion_labels`: 1 - выпот имеется; 0 - без выпота."]},{"cell_type":"code","execution_count":144,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1686298040493,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"6DP-OTsoKdch","outputId":"28f1f88e-f15a-4400-f768-8e58067903cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["['LUNG1-302' 'LUNG1-303' 'LUNG1-304' 'LUNG1-305' 'LUNG1-306']\n","['LUNG1-302' 'LUNG1-303' 'LUNG1-304' 'LUNG1-305' 'LUNG1-306']\n","[ True  True  True  True  True]\n","Всего лёгких с выпотом 121 из 121 экземпляров.\n"]}],"source":["# Создаём список меток через разницу папок в виде булевых значений\n","effusion_labels = np.isin(orig_dir_np, eff_dir_np).astype(bool)\n","\n","# Количество 3D-изображений лёгких (директорий LUNG1-XXX) - объём датасета \n","orig_elements_count = len(orig_dir_np)\n","\n","# Результат\n","print(orig_dir_np[:5])\n","print(eff_dir_np[:5])\n","print(effusion_labels[:5])\n","print(f'Всего лёгких с выпотом {np.sum(effusion_labels==1)} из {orig_elements_count} экземпляров.')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gxfFUT8DUsAR"},"source":["Получим маски для обучающей, проверочной и тестовой выборок на весь объём данных с пропорциональным разделением на лёгкие с плевральным выпотом и без него для каждого набора."]},{"cell_type":"code","execution_count":145,"metadata":{"id":"J0h3CV-0jyWa"},"outputs":[],"source":["# Функция для вывода результата разделения выборок\n","\n","def split_results(sampling_name, sampling, mask, idxs):\n","\n","    print(f'{sampling_name} выборка имеет форму {sampling.shape}')\n","    print(f'Занимает от общей выборки: {round(len(idxs)/len(effusion_labels)*100, 2)}%\\n')\n","\n","    # Проверяем, что сумма элементов маски равна количеству индексов в выборке \n","    assert mask.sum() == len(idxs)"]},{"cell_type":"code","execution_count":146,"metadata":{"id":"EFJWL8OStzUt"},"outputs":[],"source":["if not ADDITIONAL_TRAIN_f:\n","\n","    # Соотношение выборок train:val = 4:1, вычисляется из размера тестовой выборки\n","    test_size = 10\n","    val_size = (100 - test_size) / 5 if VALIDATION_f else 0\n","    train_size = 100 - test_size - val_size\n","\n","    # Процентное соотношение выборок\n","    sampling_ratio = (train_size/100, val_size/100, test_size/100)\n","    sampling_ratio"]},{"cell_type":"code","execution_count":147,"metadata":{"id":"I3Gu2JP4eaFd"},"outputs":[],"source":["if not ADDITIONAL_TRAIN_f:\n","\n","    # Получаем индексы единиц и нулей в выходном массиве\n","    ones_idxs = np.where(effusion_labels == 1)[0]\n","    zeros_idxs = np.where(effusion_labels == 0)[0]\n","\n","    # Перемешиваем индексы\n","    np.random.shuffle(ones_idxs)\n","    np.random.shuffle(zeros_idxs)\n","\n","    # Делим массив индексов единиц на 3 части\n","    ones_idxs_train, ones_idxs_val, ones_idxs_test = np.split(ones_idxs,\n","                                                              [int(len(ones_idxs)*\n","                                                                  sampling_ratio[0]),\n","                                                              int(len(ones_idxs)*\n","                                                                  (sampling_ratio[0]+\n","                                                                    sampling_ratio[1]))])\n","\n","    # Делим массив нулей на 3 части\n","    zeros_idxs_train, zeros_idxs_val, zeros_idxs_test = np.split(zeros_idxs,\n","                                                                [int(len(zeros_idxs)*\n","                                                                      sampling_ratio[0]),\n","                                                                  int(len(zeros_idxs)*\n","                                                                      (sampling_ratio[0]+\n","                                                                      sampling_ratio[1]))])\n","\n","    # Объединяем индексы из предыдущих шагов\n","    train_idxs = np.concatenate([ones_idxs_train, zeros_idxs_train])\n","    val_idxs = np.concatenate([ones_idxs_val, zeros_idxs_val])\n","    test_idxs = np.concatenate([ones_idxs_test, zeros_idxs_test])\n","\n","    # Создаём маску, которая будет соответствовать номерам элементов входного массива\n","    train_mask = np.zeros((orig_elements_count,), dtype=bool)\n","    train_mask[train_idxs] = True\n","\n","    val_mask = np.zeros((orig_elements_count,), dtype=bool)\n","    val_mask[val_idxs] = True\n","\n","    test_mask = np.zeros((orig_elements_count,), dtype=bool)\n","    test_mask[test_idxs] = True\n","\n","    # Сохраняем маски в файл NetParameters.txt для отслеживания различных результатов\n","    save_parameters(model.name, train_mask, val_mask, test_mask)\n","\n","    # Проверяем полученную форму масок\n","    assert train_mask.shape[0] == orig_elements_count"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8K_U7f45Hmx3"},"source":["## Формирование списков батчей из датасета для train, val и test выборок"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kFGEExAGhlYt"},"source":["Сформируем полные выборки директорий и меток для 3D изображений."]},{"cell_type":"code","execution_count":148,"metadata":{"id":"6X7OaENFjGWb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Тренировочная  выборка имеет форму (87,)\n","Занимает от общей выборки: 71.9%\n","\n","Проверочная  выборка имеет форму (21,)\n","Занимает от общей выборки: 17.36%\n","\n","Тестовая  выборка имеет форму (13,)\n","Занимает от общей выборки: 10.74%\n","\n"]}],"source":["# Получаем train, val и test директории и метки выборок\n","x_train, y_train = orig_dir_np[train_mask], effusion_labels[train_mask]\n","x_val, y_val = orig_dir_np[val_mask], effusion_labels[val_mask]\n","x_test, y_test = orig_dir_np[test_mask], effusion_labels[test_mask]\n","\n","if not ADDITIONAL_TRAIN_f:\n","    # Выводим результаты\n","    split_results('Тренировочная ', y_train, train_mask, train_idxs)\n","    split_results('Проверочная ', y_val, val_mask, val_idxs)\n","    split_results('Тестовая ', y_test, test_mask, test_idxs)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gTEmzXobQzNs"},"source":["Разобьём датасет на батчи:\n","\n","- зададим параметры шаг смещения `SIZE_STEP` и размер батча `BATCH_SIZE`, подготовим пустой список.\n","\n","- рассчитаем, на сколько батчей `steps` возможно разбить список директорий изображений при заданном шаге смещения. \n","\n","- в цикле возьмём нужные значения из общего списка и поместим их в батч."]},{"cell_type":"code","execution_count":149,"metadata":{"id":"k3IOUxFH9-F9"},"outputs":[],"source":["# Функция формирования батча списка названий директорий с изображениями и списка меток\n","\n","def get_batch_lists(dir_list,         # список директорий с изображениями для формирования батчей\n","                    labels,           # метки 3D изображений\n","                    batch_size,       # размер батча\n","                    size_step):       # шаг смещения\n","\n","    steps = len(dir_list)//size_step  # кол-во батчей в списке при параметрах выше\n","    list_batch_ID = []                # список для батча директорий с изображениями\n","    list_batch_labels = []            # список для батча меток, соответствующих 3D изображениям\n","\n","    for step in range(steps):\n","        \n","        # Берём элементы с нужным индексом и присоединяем их к соответствующим спискам\n","        batch_ID = dir_list[step*size_step:step*size_step + batch_size]\n","        list_batch_ID.append(batch_ID)\n","        batch_lbl = labels[step*size_step:step*size_step + batch_size]\n","        list_batch_labels.append(batch_lbl)\n","\n","    return [list_batch_ID, list_batch_labels]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4gGxA9ePAboP"},"source":["При заданных значениях шага смещения `SIZE_STEP` и батча  `BATCH_SIZE` сформируем список списков батчей директорий изображений и их меток для `train`, `val` и `test` выборок.\n","\n","Шаг смещения < размер батча можно использовать в случае аугментации изображений."]},{"cell_type":"code","execution_count":150,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686298088516,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"2OHh_GWGa0G9","outputId":"37a93284-3065-46f4-c519-36bd176f0c86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Содержимое батча директорий изображений: ['LUNG1-304']\n","Содержимое батча меток: [ True] \n","\n","Размер батча директорий изображений: 1\n","Размер батча меток: 1\n"]}],"source":["# Формируем наборы батчей для train, val и test выборок\n","batches_train = get_batch_lists(x_train, y_train, BATCH_SIZE, SIZE_STEP)\n","batches_val = get_batch_lists(x_val, y_val, BATCH_SIZE, SIZE_STEP)\n","batches_test = get_batch_lists(x_test, y_test, BATCH_SIZE, SIZE_STEP)\n","\n","# Смотрим что получилось\n","step = 1\n","print('Содержимое батча директорий изображений:', batches_train[0][step])\n","print('Содержимое батча меток:', batches_train[1][step], '\\n')\n","print('Размер батча директорий изображений:', len(batches_train[0][step]))\n","print('Размер батча меток:', len(batches_train[1][step]))"]},{"cell_type":"code","execution_count":151,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":741,"status":"ok","timestamp":1686298089252,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"Ez1iCibBbIoT","outputId":"ca4db723-ce09-471b-af76-d9f9c1f4586c"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n"]}],"source":["#@title Проверка сформированных батчей\n","def check_batches(batches):\n","    check_flags = []\n","    for i, dir in enumerate(batches[0]):\n","        if dir in orig_dir_np:\n","            idx = np.where(orig_dir_np == dir)\n","            if batches[1][i] == effusion_labels[idx]:\n","                check_flags.append(True)\n","            else: check_flags.append(False)\n","    return check_flags\n","\n","train_check = check_batches(batches_train)\n","val_check = check_batches(batches_val)\n","test_check = check_batches(batches_test)\n","\n","check_count = 0\n","check_values = True\n","for check in (train_check, val_check, test_check):\n","    check_values = check_values and all(np.unique(check))\n","    check_count += len(check)\n","\n","print(check_values and (check_count == len(orig_dir_np)))"]},{"cell_type":"code","execution_count":152,"metadata":{"cellView":"form","id":"N7o31i-JsM8z"},"outputs":[],"source":["#@title Функция для получения картинки в представлении numpy\n","\n","def get_img_np(image_path, resolution = IMG_RESOLUTION):\n","    \n","    depth_pool = () \n","    if image_path.endswith('dcm'):\n","        image_np = dicom.dcmread(image_path).pixel_array.astype('int16')\n","    elif image_path.endswith('png'):\n","        image_np = np.array(Image.open(image_path).convert('L'))\n","    else:         # endswith('nii.gz')\n","        image_np = nib.load(image_path).get_fdata().astype('int16')\n","        depth_pool = (1,)\n","\n","    # Меняем разрешение, если требуется\n","    if resolution != 512:\n","        pool_size = 1/(512/resolution)\n","        image_np = zoom(image_np, (pool_size, pool_size) + depth_pool, mode='nearest')\n","\n","    return image_np"]},{"cell_type":"code","execution_count":153,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1686298089255,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"Gs338Gi0RbiL","outputId":"5d0a1806-6afa-434c-971a-5fcfd1fb42f9"},"outputs":[{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[153], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m img \u001b[39m=\u001b[39m get_img_np(get_img_path(original_dir, i), img_res)\n\u001b[0;32m     16\u001b[0m \u001b[39mif\u001b[39;00m batches_train[\u001b[39m1\u001b[39m][step][i]: \u001b[39m# если есть выпот - получаем его маску\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     img_path \u001b[39m=\u001b[39m get_img_path(effusions_dir, i)\n\u001b[0;32m     18\u001b[0m     mask \u001b[39m=\u001b[39m get_img_np(img_path, img_res)\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m img_path\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.gz\u001b[39m\u001b[39m'\u001b[39m): mask \u001b[39m=\u001b[39m mask[:, :, mask\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m mask\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n","Cell \u001b[1;32mIn[153], line 8\u001b[0m, in \u001b[0;36mget_img_path\u001b[1;34m(dir, i)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_img_path\u001b[39m(\u001b[39mdir\u001b[39m, i):\n\u001b[0;32m      6\u001b[0m     img_dir \u001b[39m=\u001b[39m get_full_path(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mdir\u001b[39m, batches_train[\u001b[39m0\u001b[39m][step][i]))               \u001b[39m# путь к папке со срезами\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(img_dir,\n\u001b[1;32m----> 8\u001b[0m                             \u001b[39msorted\u001b[39;49m(os\u001b[39m.\u001b[39;49mlistdir(img_dir))[\u001b[39mlen\u001b[39;49m(os\u001b[39m.\u001b[39;49mlistdir(img_dir)) \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m2\u001b[39;49m]) \u001b[39m# средний срез\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m img_path\n","\u001b[1;31mIndexError\u001b[0m: list index out of range"]}],"source":["#@title Посмотрим, что получилось на батче на шаге step\n","\n","img_res = 128\n","\n","def get_img_path(dir, i):\n","    img_dir = get_full_path(os.path.join(dir, batches_train[0][step][i]))               # путь к папке со срезами\n","    img_path = os.path.join(img_dir,\n","                            sorted(os.listdir(img_dir))[len(os.listdir(img_dir)) // 2]) # средний срез\n","    return img_path\n","\n","# Сформируем numpy для изображения и маски\n","for i in range(BATCH_SIZE):\n","\n","    img = get_img_np(get_img_path(original_dir, i), img_res)\n","\n","    if batches_train[1][step][i]: # если есть выпот - получаем его маску\n","        img_path = get_img_path(effusions_dir, i)\n","        mask = get_img_np(img_path, img_res)\n","        if img_path.endswith('.gz'): mask = mask[:, :, mask.shape[2] - mask.shape[2] // 2 - 1]\n","        mask_name = 'Есть маска'\n","    else:                          # если нет - формируем пустое изображение\n","        mask = np.zeros((img_res, img_res))\n","        mask_name = 'Нет маски'\n","\n","    plot_pics((img, mask), ('Оригинальное изображение', 'Маска выпота'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LXbFi81afEcq"},"source":["##Генерация батчей с данными"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NiDsm2UEfBt2"},"source":["Добавим недостающие директории в датасет с выпотами для равного количества оригинальных и сегментированных экземпляров."]},{"cell_type":"code","execution_count":154,"metadata":{"id":"D7BHz7loXqgZ"},"outputs":[],"source":["for dir in np.setdiff1d(orig_dir_np, eff_dir_np):\n","    if not os.path.exists(os.path.join(effusions_dir, dir)):\n","        os.makedirs(os.path.join(effusions_dir, dir))"]},{"cell_type":"code","execution_count":155,"metadata":{"cellView":"form","id":"iPkqtosccoxZ"},"outputs":[],"source":["#@title Дополнительные функции\n","\n","# Функции преобразования сегментированного изображения в метки классов и обратно при 2D или упрощённой 3D сегментации\n","def bool_to_labels(image, lung_folder): # 2D или 3D картинка для преобразования\n","\n","    class_lbl = np.where(eff_dir_np == lung_folder)[0]\n","    image[np.where(np.all(image == 0))] = class_lbl * 2\n","    image[np.where(np.all(image == 1))] = class_lbl * 2 + 1\n","  \n","    return image\n","\n","\n","def labels_to_bool(image):\n","\n","    class_lbl = np.unique(image)[0]//2    \n","    image[np.where(np.all(image == class_lbl * 2))] = 0\n","    image[np.where(np.all(image == class_lbl * 2 + 1))] = 1\n","\n","    return image\n","\n","  \n","# Функция ограничения значений пикселей в предполагаемых пределах выпота для повышения точности\n","\n","def apply_bounds(image_3d, image):\n","\n","    image_hu = image_3d * np.int16(image.RescaleSlope) + np.int16(image.RescaleIntercept)\n","    image_hu[image_hu>MAX_BOUND] = MAX_BOUND\n","    image_hu[image_hu<MIN_BOUND] = MIN_BOUND\n","    image_3d = ((image_hu - np.int16(image.RescaleIntercept))/np.int16(image.RescaleSlope)).astype(np.int16)\n","\n","    return image_3d\n","\n","\n","# Функция определения позиции окна при разделении 3D изображения на части\n","\n","def get_chunk_position(num, # номер обрабатываемой части\n","                       n):  # количество частей\n","    row = int(num // np.sqrt(n))\n","    col = int(num % np.sqrt(n))\n","    return (row, col)"]},{"cell_type":"code","execution_count":156,"metadata":{"cellView":"form","id":"t14FcfKCGb_3"},"outputs":[],"source":["#@title Генератор батчей\n","\n","# Функция для получения numpy массива 3D изображений лёгкого\n","\n","def get_3d_image(path_dir,                    # путь к датасету (оригинальные/маски)\n","                 lungfolder,                  # название папки экземпляра\n","                 chunks_count,                # количество срезов в 3D изображении\n","                 simple3d_batch_num,          # номер батча в случае упрощённой 3D сегментации\n","                 chunk_res):\n","\n","        image_3d_np = np.zeros((chunk_res, chunk_res, MAX_SLICES),    # пустой 3D массив\n","                               dtype = np.int16)\n","\n","        chunk_row, chunk_col = get_chunk_position(simple3d_batch_num, chunks_count)\n","        full_path = get_full_path(os.path.join(path_dir, lungfolder))           # путь к папке с изображениями\n","        image_files = sorted(os.listdir(full_path))                             # изображения в папке экземпляра\n","        images_count = len(image_files)                                         # количество изображений в папке экземпляра\n","        \n","        # Если каталог не пуст (создан при распаковке, не программно)\n","        if images_count > 0:\n","\n","\n","#доработать части\n","\n","\n","            if image_files[0].endswith('.gz'):\n","                # получаем 3D массив маски экземпляра\n","                image_path = os.path.join(full_path, image_files[0])\n","                nii_img = get_img_np(image_path)\n","                # поворачиваем его против часовой, ставим обратный порядок срезов и вставляем в image_3d_np\n","                image_3d_np[:, :, :nii_img.shape[2]] = np.rot90(nii_img)[:, :, ::-1]\n","\n","            else:           # endswith '.dcm' or '.png'                  \n","                # для всех файлов в каталоге по указанному пути:\n","                for slice_num in range(images_count):\n","                    # читаем 2D картинку и берём из неё нужную часть\n","                    image_path = os.path.join(full_path, image_files[slice_num])\n","                    img_np = get_img_np(image_path)[chunk_row * chunk_res:(chunk_row+1) * chunk_res,\n","                                                    chunk_col * chunk_res:(chunk_col+1) * chunk_res]\n","                    # добавляем её в 3D массив\n","                    image_3d_np[:, :, slice_num] = img_np\n","\n","            # Для оригинальных (DICOM) изображений применяем границы пикселей по HU\n","            if image_path.endswith('dcm'):\n","                image_3d_np = apply_bounds(image_3d_np, dicom.dcmread(image_path))\n","\n","            # В случае 2D или упрощённой 3D сегментации - переводим булевую маску в метки экземпляров\n","            #if SIMPLE_3D_f and image_path.endswith('png'): image_3d_np = bool_to_labels(image_3d_np, lungfolder)\n","        \n","        return image_3d_np\n","\n","\n","def batch_generator(step,                                   # шаг обучения\n","                    set_batches,                            # список из списков батчей директорий с изображениями и списка меток\n","                    chunks_count = 1,                       # количество частей, на которые поделён 2D срез 3D изображения\n","                    batch_size = BATCH_SIZE,                # размер батча для генератора\n","                    simple3d_batch_num = 0,                 # номер батча в случае упрощённой 3D сегментации\n","                    chunk_res = IMG_RESOLUTION):\n","\n","    # Создаём пустые 5D-массивы\n","    x_batch_np = np.zeros((batch_size, chunk_res, chunk_res, MAX_SLICES, 1), dtype = np.int16)\n","    y_batch_np = np.zeros((batch_size, chunk_res, chunk_res, MAX_SLICES, 1), dtype = np.int16)\n","\n","    # Для всех директорий в каталоге по указанному пути:\n","    for i, lung_folder in enumerate(set_batches[0][step]):   # список батча директорий с изображениями\n","\n","        # Записываем 3D-массив экземпляра в нужный элемент 5D массива x_batch_np\n","        np.copyto(x_batch_np[i, :, :, :, 0], get_3d_image(original_dir, lung_folder, chunks_count, simple3d_batch_num, chunk_res))\n","\n","        # Если есть выпот - закидываем картинки в 3D изображение и вставляем его в 5D массив y_batch_np\n","        if set_batches[1][step][i]: # метка выпота\n","            np.copyto(y_batch_np[i, :, :, :, 0], get_3d_image(effusions_dir, lung_folder, chunks_count, simple3d_batch_num, chunk_res))\n","\n","    # Нормализуем данные батча оригинальных изображений к значениям от 0 до 1\n","    x_batch_np = (x_batch_np.astype(np.float16) - np.float16(MIN_BOUND)) / (np.float16(MAX_BOUND) - np.float16(MIN_BOUND))\n","\n","    # Возвращаем батч нормализованных данных 3D изображений\n","    return (x_batch_np, y_batch_np)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-Zz1m-C33JXz"},"source":["###Проверим работу генератора"]},{"cell_type":"code","execution_count":157,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5575,"status":"ok","timestamp":1686298094812,"user":{"displayName":"Max Pisarev","userId":"03484773034450068495"},"user_tz":-180},"id":"dD7KwwMqFPsY","outputId":"0bc4dac4-2409-4ba2-a8f8-52581da40932"},"outputs":[{"name":"stdout","output_type":"stream","text":["float16 (1, 16, 16, 304, 1) (1, 16, 16, 304, 1)\n","Изображения Оригинальные тренировочные загружены.\n","Время загрузки: 0 мин 0 с\n","Директории: ['LUNG1-302']\n","Метки выпота: [ True]\n"]}],"source":["# Отметка текущего времени\n","cur_time = datetime.datetime.now()\n","\n","# Формируем батч тренировочной выборки\n","step = 0\n","if SIMPLE_3D_f:\n","    batch_step_train = batch_generator(step, batches_train,\n","                                       chunks_count, BATCH_SIZE)\n","else:\n","    batch_step_train = batch_generator(step, batches_train)\n","\n","print(batch_step_train[0].dtype, batch_step_train[0].shape, batch_step_train[1].shape)\n","\n","# Получаем время обработки\n","time_diff = datetime.datetime.now() - cur_time\n","\n","print('Изображения Оригинальные тренировочные загружены.')\n","print(f'Время загрузки: {(time_diff.seconds//60)%60} мин {time_diff.seconds%60} с')\n","print('Директории:', batches_train[0][step])\n","print('Метки выпота:', batches_train[1][step])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CcKbpd_fb29d"},"source":["Посмотрим картинки из батча (с выпотом и без). Первые две - оригинальные изображения, вторые - маски к ним.\n","\n","Для этого в полученных на предыдущем шаге метках должны присутствовать False и True и BATCH_SIZE должен быть больше 1. Если их нет - необходимо изменить параметр step."]},{"cell_type":"code","execution_count":158,"metadata":{"id":"5-SuF3DycFhA"},"outputs":[],"source":["if BATCH_SIZE > 1 and len(np.unique(batches_train[1][step])) > 1:\n","    print('IMG_RESOLUTION:', IMG_RESOLUTION)\n","\n","    slice_num = 64  # Номер среза\n","    idx = []        # Индексы экземпляра с выпотом idx[0] и без idx[1]\n","\n","    idx.append(np.where(batches_train[1][step])[0])\n","    idx.append(np.where(~batches_train[1][step])[0])\n","    images_name = ('Оригинальные изображения', 'Маски')\n","\n","    for i in range(2):\n","\n","        # Получаем данные изображений\n","        img1_np = batch_step_train[i][idx[0][0],:,:,slice_num].squeeze(axis=2)\n","        img2_np = batch_step_train[i][idx[1][0],:,:,slice_num].squeeze(axis=2)\n","\n","        # Выводим изображения\n","        print(images_name[i])\n","        plot_pics((img1_np, img2_np), ('С выпотом', 'Без выпота'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f1xPeV8paFuk"},"source":["На выходе мы имеем готовый батч в виде массивов оригинальных и сегментированных 3D изображений для подачи на обучение.\n","\n","Формы массивов (None, 512, 512, 304, 1) (для полной 3D сегментации):\n","\n","(размер батча, ширина картинки, высота картинки, количество срезов, количество каналов).\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2PxXko3FvCfA"},"source":["##Обучение Train on batch"]},{"cell_type":"code","execution_count":159,"metadata":{"cellView":"form","id":"Yinr-RYjvP3H"},"outputs":[],"source":["#@title Train Eval step\n","\n","def train_eval_step(step, step_data, steps, params, mode_str, int_start, batch_num = 0):\n","\n","    # создаём пустые списки для сбора данных по шагам внутри эпохи\n","    loss = []\n","    biou = []\n","    dice = []\n","    #acc = []\n","    \n","    # присваиваем наименования ключам в словаре\n","    loss_str = mode_str + 'loss'\n","    biou_str = mode_str + 'BinaryIoU'\n","    dice_str = mode_str + 'dice'\n","    #accuracy_str = mode_str + 'accuracy'\n","\n","    # train mode\n","    if mode_str == '':\n","        # пропускаем проверочный батч данных через функцию оценки модели,\n","        # получаем ошибку и точность как список\n","        result_step = model.train_on_batch(step_data[0], step_data[1])\n","        # задаём параметры в словаре, которые будем выводить\n","        time_diff = datetime.datetime.now() - int_start     # получаем время обработки\n","        params = {\n","            'Время(сек.) на эпохе': time_diff.seconds,      # считаем время обучения на данной эпохе и добавляем в словарь\n","            loss_str: round(result_step[0], 4),             # добавляем в словарь ошибку на шаге обучения\n","            biou_str: round(result_step[1], 4),\n","            dice_str: round(result_step[2], 4)\n","        }\n","        # печатаем отдельной функцией текущие данные на шаге обучения\n","        print_log(step * chunks_count + batch_num, steps * chunks_count, params)\n","\n","    # evaluate mode\n","    else:\n","        # оцениваем модель на проверочной базе\n","        result_step = model.evaluate(step_data[0], step_data[1], verbose=0)\n","\n","    # собираем ошибку и точность на шаге\n","    loss.append(result_step[0])\n","    biou.append(result_step[1])\n","    dice.append(result_step[2])\n","\n","    return model, params, loss, biou, dice"]},{"cell_type":"code","execution_count":160,"metadata":{"cellView":"form","id":"SQIHjyXz-qtN"},"outputs":[],"source":["#@title Train Eval mode\n","def train_eval_mode(model,                  # модель\n","                    steps,                  # количество шагов на эпохе\n","                    batches,                # батчи директорий и меток\n","                    loss_lrn,               # список для сбора усреднённых ошибок в конце эпохи\n","                    biou_lrn,\n","                    dice_lrn,\n","                    int_start=None,         # начальное время эпохи\n","                    params=None,            # словарь параметров эпохи\n","                    mode_str=''):           # режим обучения ('') и проверки ('val_')\n","\n","    # присваиваем наименования ключам в словаре\n","    loss_str = mode_str + 'loss'\n","    biou_str = mode_str + 'binary_io_u'\n","    dice_str = mode_str + 'dice'\n","    #accuracy_str = mode_str + 'accuracy'\n","\n","    for step in range(steps):   \n","        if SIMPLE_3D_f:\n","            step_data_2d = []               # данные в формате (slices_count, img_res, img_res, 1)\n","            for batch_num in range(chunks_count):\n","                step_data = batch_generator(step, batches, chunks_count, 1, batch_num, chunk_res)\n","                #print(step, batch_num, step_data[0].shape, step_data[1].shape)\n","                '''for i in range(2):\n","                    # получаем массив оригинального/сегментированного изображения\n","                    step_data_2d.append(batch3d_to2d(step_data[i]))\n","                    ниже это выше batch3d_to2d\n","                    # Убираем единичные оси и конвертируем массив под формат данных для 2D сети\n","                    array = np.transpose(np.squeeze(array), (2, 0, 1))\n","                    \n","                    \n","                    # создаём массив предсказаний модели на основе срезов 2D изображений\n","                    predictions = np.zeros((MAX_SLICES, IMG_RESOLUTION, IMG_RESOLUTION, 1), dtype = np.int16)\n","                    # каждый срез в 3d массиве\n","                    for slice_num in range(step_data_2d[i].shape[0]):\n","                        # пропускаем через обученную 2D модель и получаем предсказание\n","                        # ********************************* model_2d\n","                        prediction_2d = model.predict(step_data_2d[i][slice_num])\n","                        x_batch_np = np.zeros((batch_size, IMG_RESOLUTION, IMG_RESOLUTION, slices_count, 1), dtype = np.int16)\n","                        np.copyto(y_batch_np[i, :, :, :, 0], get_3d_image(effusions_dir, lung_folder, slices_count, simple3d_batch_num))\n","                    for i in range(original_volume.shape[0]):\n","\n","                        # сохранить предсказание в список\n","                        predictions.append(prediction_2d)\n","\n","                        # объединить предсказания для каждого среза в одно 3D изображение\n","                        prediction_volume = np.concatenate(predictions, axis=0)\n","\n","                        # объединить полученное 3D изображение с оригинальным 3D изображением и сегментированным 3D изображением\n","                        merged_volume = np.stack([original_volume, segmented_volume, prediction_volume], axis=3)\n","\n","                        # использовать полученные данные для обучения 3D модели'''\n","\n","\n","\n","                model, params, loss, biou, dice = train_eval_step(step, step_data, steps, params, mode_str, int_start, batch_num)\n","        else:\n","            # получаем батч данных на этапе\n","            step_data = batch_generator(step, batches)\n","            model, params, loss, biou, dice = train_eval_step(step, step_data, steps, params, mode_str, int_start)\n","\n","    # собираем усреднённые ошибку и точность от всех шагов на эпохе\n","    loss_lrn.append(np.mean(loss))\n","    biou_lrn.append(np.mean(biou))\n","    dice_lrn.append(np.mean(dice))\n","\n","    # запоминаем и округляем последнее записанное усреднённое значение для отображения по ходу обучения\n","    # в train mode перезаписываем значения в словаре, в evaluate mode добавляем данные в словарь\n","    params.update({\n","        loss_str: round(loss_lrn[-1],4),\n","        biou_str: round(biou_lrn[-1],4),\n","        dice_str: round(dice_lrn[-1],4)\n","    })\n","\n","    return model, (loss_lrn, biou_lrn, dice_lrn, params, model)"]},{"cell_type":"code","execution_count":161,"metadata":{"cellView":"form","id":"odaT8j1N9Sk8"},"outputs":[],"source":["#@title Model train on batch\n","\n","def model_train_on_batch(model, optimizer=optimizer, epochs=50, last_epoch=0):\n","\n","    # создаём пустые списки для сбора усредненных данных от шагов в конце эпохи\n","    loss_train_lrn =[]\n","    biou_train_lrn = []\n","    dice_train_lrn = []\n","    loss_val_lrn =[]\n","    biou_val_lrn = []\n","    dice_val_lrn = []\n","\n","    # определяем количество шагов\n","    steps_train = len(batches_train[0])\n","    steps_val = len(batches_val[0])\n","\n","    callback = {\n","        'mc_loss': 100, 'mc_accuracy': 0, 'mc_epoch': 0,\n","        'lr_count': 0, 'lr_epoch': 0, 'lr_loss': 0,\n","        'es_count': 0, 'es_epoch': 0, 'es_loss': 0,\n","    }\n","\n","    # запоминаем время старта обучения\n","    # start_time = datetime.datetime.now()\n","\n","    # запускаем цикл обучения по эпохам\n","    for epoch in range(epochs):\n","        \n","        # выводим текущую эпоху и общее число эпох\n","        print('Эпоха', last_epoch+epoch+1, '/', last_epoch+epochs)\n","\n","        # запускаем цикл обучения по шагам внутри эпохи\n","        model, train_params = train_eval_mode(model, steps_train, batches_train,\n","                                              loss_train_lrn, biou_train_lrn, dice_train_lrn,\n","                                              datetime.datetime.now())\n","\n","        if VALIDATION_f:\n","            # оцениваем модель на проверочной базе\n","            model, val_params = train_eval_mode(model, steps_val, batches_val,\n","                                                loss_val_lrn, biou_val_lrn, dice_val_lrn,\n","                                                params=train_params[3], mode_str='val_')\n","\n","        # печатаем отдельной функцией усреднённые данные в конце текущей эпохи обучения\n","        print_params = val_params[3] if VALIDATION_f else train_params[3]\n","        print_log(steps_train * chunks_count, steps_train * chunks_count, print_params)\n","\n","        # Вручную переносим каретку на следующую строку,\n","        # чтобы не стирать финальные значения сети на эпохе\n","        print()\n","\n","        model_history = {\n","            'loss': train_params[0],\n","            'binary_io_u': train_params[1],\n","            'dice': train_params[2]\n","        }\n","        \n","        if VALIDATION_f:\n","            model_history.update({\n","                'val_loss': val_params[0],\n","                'val_binary_io_u': train_params[1],\n","                'dice': train_params[2]\n","            })\n","\n","        model.save_weights(os.path.join(GDRIVE_PATH, DRIVE_DATA_DIR, f'{model.name}_{str(last_epoch+epoch+1)}_weights.h5'))\n","        model.save(os.path.join(GDRIVE_PATH, DRIVE_DATA_DIR, f'{model.name}_{str(last_epoch+epoch+1)}_model.h5'))\n","        \n","        '''model, callback = callbacks(model, model_name, epoch, model_history, callback)\n","        if callback['es_count'] == -1: break'''\n","\n","    return model, model_history"]},{"cell_type":"code","execution_count":162,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MQkpHpJL7yMa","outputId":"0e927535-b7c7-4afe-ca17-71e5f175b559"},"outputs":[{"name":"stdout","output_type":"stream","text":["IMG_RESOLUTION = 16\n","BATCH_SIZE = 1\n","Model: unet_3d\n","Chunks count & resolution: 1 16\n","Эпоха 1 / 50\n","11/87 [==>-----------------]  Время(сек.) на эпохе: 117. loss: 1.0. BinaryIoU: 0.4551. dice: 0.0. 42. "]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[162], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mChunks count & resolution:\u001b[39m\u001b[39m'\u001b[39m, chunks_count, chunk_res)\n\u001b[0;32m      6\u001b[0m \u001b[39m# Обучаем модель на батчах\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model, model_h \u001b[39m=\u001b[39m model_train_on_batch(model, optimizer, last_epoch \u001b[39m=\u001b[39;49m LAST_EPOCH)\n","Cell \u001b[1;32mIn[161], line 33\u001b[0m, in \u001b[0;36mmodel_train_on_batch\u001b[1;34m(model, optimizer, epochs, last_epoch)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mЭпоха\u001b[39m\u001b[39m'\u001b[39m, last_epoch\u001b[39m+\u001b[39mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, last_epoch\u001b[39m+\u001b[39mepochs)\n\u001b[0;32m     32\u001b[0m \u001b[39m# запускаем цикл обучения по шагам внутри эпохи\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model, train_params \u001b[39m=\u001b[39m train_eval_mode(model, steps_train, batches_train,\n\u001b[0;32m     34\u001b[0m                                       loss_train_lrn, biou_train_lrn, dice_train_lrn,\n\u001b[0;32m     35\u001b[0m                                       datetime\u001b[39m.\u001b[39;49mdatetime\u001b[39m.\u001b[39;49mnow())\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m VALIDATION_f:\n\u001b[0;32m     38\u001b[0m     \u001b[39m# оцениваем модель на проверочной базе\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     model, val_params \u001b[39m=\u001b[39m train_eval_mode(model, steps_val, batches_val,\n\u001b[0;32m     40\u001b[0m                                         loss_val_lrn, biou_val_lrn, dice_val_lrn,\n\u001b[0;32m     41\u001b[0m                                         params\u001b[39m=\u001b[39mtrain_params[\u001b[39m3\u001b[39m], mode_str\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m)\n","Cell \u001b[1;32mIn[160], line 60\u001b[0m, in \u001b[0;36mtrain_eval_mode\u001b[1;34m(model, steps, batches, loss_lrn, biou_lrn, dice_lrn, int_start, params, mode_str)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m         \u001b[39m# получаем батч данных на этапе\u001b[39;00m\n\u001b[0;32m     59\u001b[0m         step_data \u001b[39m=\u001b[39m batch_generator(step, batches)\n\u001b[1;32m---> 60\u001b[0m         model, params, loss, biou, dice \u001b[39m=\u001b[39m train_eval_step(step, step_data, steps, params, mode_str, int_start)\n\u001b[0;32m     62\u001b[0m \u001b[39m# собираем усреднённые ошибку и точность от всех шагов на эпохе\u001b[39;00m\n\u001b[0;32m     63\u001b[0m loss_lrn\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(loss))\n","Cell \u001b[1;32mIn[159], line 21\u001b[0m, in \u001b[0;36mtrain_eval_step\u001b[1;34m(step, step_data, steps, params, mode_str, int_start, batch_num)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m#accuracy_str = mode_str + 'accuracy'\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39m# train mode\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mif\u001b[39;00m mode_str \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     19\u001b[0m     \u001b[39m# пропускаем проверочный батч данных через функцию оценки модели,\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[39m# получаем ошибку и точность как список\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     result_step \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_on_batch(step_data[\u001b[39m0\u001b[39;49m], step_data[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     22\u001b[0m     \u001b[39m# задаём параметры в словаре, которые будем выводить\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     time_diff \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m int_start     \u001b[39m# получаем время обработки\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2510\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   2506\u001b[0m     iterator \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39msingle_batch_iterator(\n\u001b[0;32m   2507\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy, x, y, sample_weight, class_weight\n\u001b[0;32m   2508\u001b[0m     )\n\u001b[0;32m   2509\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 2510\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   2512\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m return_dict:\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mc:\\Users\\MaxHari\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print('IMG_RESOLUTION =', IMG_RESOLUTION)\n","print('BATCH_SIZE =', BATCH_SIZE)\n","print(f'Model: {model.name}')\n","print('Chunks count & resolution:', chunks_count, chunk_res)\n","\n","# Обучаем модель на батчах\n","model, model_h = model_train_on_batch(model, optimizer, last_epoch = LAST_EPOCH)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["hV1xU4xu56CT","gWYQXtkIlMNQ","Lxyoy71OGPON","LXbFi81afEcq","-Zz1m-C33JXz","qs5Y0xreslnb"],"gpuType":"T4","provenance":[{"file_id":"1T0pavVnK4_I9lMehFrL6OhSKJN0dB-L2","timestamp":1685963556262},{"file_id":"1GadOLeeXQgo30mjg5iodM2wtGvuKEEob","timestamp":1685963236903},{"file_id":"1-LGRS4yAq9F45rqiiJjfEyjQbEeyLHHm","timestamp":1685953672728},{"file_id":"14xLY3XCY2Nxl8N6P_HI64Iin9wJbvqZH","timestamp":1685953093006},{"file_id":"15hm91e1VuHeJmy5ANrctZmdCctEWScCl","timestamp":1685941690345}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
